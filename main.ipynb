{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82bdeb0f",
   "metadata": {
    "id": "82bdeb0f"
   },
   "source": [
    "# CS-7643 Deep Learning Final Project\n",
    "\n",
    "### by Zhiyin (Steven) Lu and Yixuan (Elliot) Xie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8050e28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1659336171426,
     "user": {
      "displayName": "Steven Lu",
      "userId": "02385023182250863259"
     },
     "user_tz": 300
    },
    "id": "e8050e28",
    "outputId": "24804067-c0d8-48b7-f8cf-02f99bdededd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# %cd drive/MyDrive/cs7643_final_project\n",
    "%cd drive/MyDrive/cs7643\n",
    "%ls\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92e22d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6513,
     "status": "ok",
     "timestamp": 1659336177937,
     "user": {
      "displayName": "Steven Lu",
      "userId": "02385023182250863259"
     },
     "user_tz": 300
    },
    "id": "2d92e22d",
    "outputId": "daa3476b-33ab-4578-d9f7-4a51d45cf184"
   },
   "outputs": [],
   "source": [
    "pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821e3eb",
   "metadata": {
    "id": "e821e3eb"
   },
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60657bed",
   "metadata": {
    "id": "60657bed"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold # need scikit-learn 1.1.1\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fbd853",
   "metadata": {
    "id": "71fbd853"
   },
   "source": [
    "## 1. Build Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90695b29",
   "metadata": {
    "id": "90695b29"
   },
   "source": [
    "### 1.1 Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53528e",
   "metadata": {
    "id": "3a53528e"
   },
   "outputs": [],
   "source": [
    "class Transform():\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def baseline(self):\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(*self.img_size)\n",
    "        ])\n",
    "        valid_transform = A.Compose([\n",
    "            A.Resize(*self.img_size)\n",
    "        ])\n",
    "        data_transform = {'train': train_transform, 'valid': valid_transform}\n",
    "        return data_transform\n",
    "    \n",
    "    def augmentation(self):\n",
    "        train_transform = A.Compose([\n",
    "        A.Resize(*self.img_size),\n",
    "        A.ShiftScaleRotate(shift_limit=0., scale_limit=0.1, rotate_limit=10, border_mode=0, p=0.5),\n",
    "        A.RandomCrop(255, 255)\n",
    "        ])  \n",
    "\n",
    "        valid_transform = A.Compose([\n",
    "            A.Resize(*self.img_size)\n",
    "        ])\n",
    "        data_transform = {'train': train_transform, 'valid': valid_transform}\n",
    "        \n",
    "        return data_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8189e",
   "metadata": {
    "id": "16d8189e"
   },
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a1c27",
   "metadata": {
    "id": "ac2a1c27"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, backbone, pretrained_weights, num_class, device):\n",
    "        self.backbone = backbone\n",
    "        self.pretrained_weights = pretrained_weights\n",
    "        self.num_class = num_class\n",
    "        self.device = device\n",
    "    \n",
    "    def UNet(self):\n",
    "        model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.pretrained_weights,\n",
    "            in_channels=3,\n",
    "            classes=self.num_class,\n",
    "            activation=None\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def UNetPlusPlus(self):\n",
    "        model = smp.UnetPlusPlus(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.pretrained_weights,\n",
    "            in_channels=3,\n",
    "            classes=self.num_class,\n",
    "            activation=None\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa7ac2",
   "metadata": {
    "id": "ddaa7ac2"
   },
   "source": [
    "### 1.3 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541fff6",
   "metadata": {
    "id": "f541fff6"
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def BCE(self):\n",
    "        return smp.losses.SoftBCEWithLogitsLoss()\n",
    "    \n",
    "    def Tversky(self):\n",
    "        return smp.losses.TverskyLoss(mode='multilabel', log_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd17f85",
   "metadata": {
    "id": "7cd17f85"
   },
   "source": [
    "### 1.4 Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e421c7",
   "metadata": {
    "id": "d7e421c7"
   },
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    def __init__(self, threshold, epsilon):\n",
    "        self.threshold = threshold\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def dice(self, y_pred, y_true):\n",
    "        y_pred, y_true = (y_pred > self.threshold).to(torch.float32), y_true.to(torch.float32)\n",
    "        inter = (y_pred * y_true).sum(dim=(-2, -1))\n",
    "        score = (2 * inter + self.epsilon) / (y_true.sum(dim=(-2, -1)) + y_pred.sum(dim=(-2, -1)) + self.epsilon)\n",
    "        score = score.mean(dim=(1, 0))\n",
    "        return score\n",
    "    \n",
    "    def IOU(self, y_pred, y_true):\n",
    "        y_pred, y_true = (y_pred > self.threshold).to(torch.float32), y_true.to(torch.float32)\n",
    "        inter = (y_pred * y_true).sum(dim=(-2, -1))\n",
    "        union = (y_pred + y_true - y_pred * y_true).sum(dim=(-2, -1))\n",
    "        score = (inter + self.epsilon) / (union + self.epsilon)\n",
    "        score = score.mean(dim=(1, 0))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7b57",
   "metadata": {
    "id": "a3cf7b57"
   },
   "source": [
    "### 1.5 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddac23b",
   "metadata": {
    "id": "1ddac23b"
   },
   "outputs": [],
   "source": [
    "# class Dataset(Dataset):\n",
    "#     def __init__(self, df, transform):\n",
    "#         self.df = df\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         scan_path = self.df['scan_path'][index]\n",
    "#         scan = cv2.imread(scan_path, -1)\n",
    "#         # 3 channels for RGB\n",
    "#         scan = np.tile(scan[..., None], [1, 1, 3]).astype('float32')\n",
    "#         # normalize\n",
    "#         mx = np.max(scan)\n",
    "#         if mx:\n",
    "#             scan /= mx\n",
    "        \n",
    "#         mask_path = self.df['mask_path'][index]\n",
    "#         mask = np.load(mask_path).astype('float32')\n",
    "#         mask /= 255.0\n",
    "        \n",
    "#         transformed = self.transform(image=scan, mask=mask)\n",
    "#         scan, mask = transformed['image'], transformed['mask']\n",
    "#         scan = torch.tensor(np.transpose(scan, (2, 0, 1)))\n",
    "#         mask = torch.tensor(np.transpose(mask, (2, 0, 1)))\n",
    "        \n",
    "#         return scan, mask\n",
    "\n",
    "\n",
    "# For 2.5d data\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        scan_path = self.df['scan_path'][index]\n",
    "        scan = np.load(scan_path).astype('float32')\n",
    "        mx = np.max(scan)\n",
    "        if mx:\n",
    "            scan /= mx\n",
    "        \n",
    "        mask_path = self.df['mask_path'][index]\n",
    "        mask = np.load(mask_path).astype('float32')\n",
    "        mask /= 255.0\n",
    "        \n",
    "        transformed = self.transform(image=scan, mask=mask)\n",
    "        scan, mask = transformed['image'], transformed['mask']\n",
    "        scan = torch.tensor(np.transpose(scan, (2, 0, 1)))\n",
    "        mask = torch.tensor(np.transpose(mask, (2, 0, 1)))\n",
    "        \n",
    "        return scan, mask\n",
    "    \n",
    "def get_dataloader(df, fold, data_transform, train_batch_size, valid_batch_size, num_workers):\n",
    "    train_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    train_dataset = Dataset(train_df, transform=data_transform['train'])\n",
    "    valid_dataset = Dataset(valid_df, transform=data_transform['valid'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, num_workers=num_workers, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440feb6",
   "metadata": {
    "id": "7440feb6"
   },
   "source": [
    "### 1.6 Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22904cb4",
   "metadata": {
    "id": "22904cb4"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    scaler = amp.GradScaler()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Training ...')\n",
    "    for i, sample in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        scans, masks = sample\n",
    "        scans, masks = scans.to(device, dtype=torch.float), masks.to(device, dtype=torch.float)\n",
    "        with amp.autocast():\n",
    "            y_preds = model(scans)\n",
    "            bce_loss = loss_fn.BCE()(y_preds, masks)\n",
    "            tverskly_loss = loss_fn.Tversky()(y_preds, masks)\n",
    "            loss = 0.5 * bce_loss + 0.5 * tverskly_loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() / scans.shape[0]\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'CURRENT LEARNING RATE: {curr_lr:.3f}, TOTAL LOSS: {total_loss:.3f}', flush=True)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(valid_loader, model, metric, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    pbar = tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Validating ...')\n",
    "    for i, sample in pbar:\n",
    "        scans, masks = sample\n",
    "        scans, masks = scans.to(device, dtype=torch.float), masks.to(device, dtype=torch.float)\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        y_preds = sigmoid(model(scans))\n",
    "        dice_score = metric.dice(y_preds, masks).cpu().detach().numpy()\n",
    "        iou_score = metric.IOU(y_preds, masks).cpu().detach().numpy()\n",
    "        scores.append([dice_score, iou_score])\n",
    "    scores = np.mean(scores, axis=0)\n",
    "    print(f'DICE SCORE: {scores[0]:.3f}, IOU SCORE: {scores[1]:.3f}', flush=True)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9420ef7",
   "metadata": {
    "id": "a9420ef7"
   },
   "source": [
    "## 2. Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd83d5c",
   "metadata": {
    "id": "9bd83d5c"
   },
   "source": [
    "### 2.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205ee4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1659336178097,
     "user": {
      "displayName": "Steven Lu",
      "userId": "02385023182250863259"
     },
     "user_tz": 300
    },
    "id": "d205ee4b",
    "outputId": "b0913a45-a581-4d64-e059-a7a4550175a1"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_FOLD = 3\n",
    "EPOCH = 10\n",
    "IMG_SIZE = (352, 352)\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Model\n",
    "BACKBONE = 'efficientnet-b1'\n",
    "PRETRAINED_WEIGHTS = 'imagenet'\n",
    "NUM_CLASS = 3\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# LEARNING_RATE_DROP = 2\n",
    "\n",
    "# Metric\n",
    "THRESHOLD = 0.45\n",
    "EPSILON = 0.001\n",
    "\n",
    "# Device and Directory\n",
    "EXPERIMENT_NAME = 'UNet_2.5d'\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CKPT_SAVE_PATH = f'./checkpoints/{EXPERIMENT_NAME}_batchsize{TRAIN_BATCH_SIZE}_fold{NUM_FOLD}'\n",
    "if not os.path.exists(CKPT_SAVE_PATH):\n",
    "    os.makedirs(CKPT_SAVE_PATH)\n",
    "if not os.path.exists(f'{CKPT_SAVE_PATH}/trained'):\n",
    "    os.makedirs(f'{CKPT_SAVE_PATH}/trained')\n",
    "if not os.path.exists(f'{CKPT_SAVE_PATH}/best'):\n",
    "    os.makedirs(f'{CKPT_SAVE_PATH}/best')\n",
    "print(f'EXPERIMENT NAME: {EXPERIMENT_NAME}')\n",
    "print(f'CURRENT DEVICE: {DEVICE}, CHECKPOINT SAVE PATH: {CKPT_SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429f17f",
   "metadata": {
    "id": "c429f17f"
   },
   "source": [
    "### 2.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab67f65",
   "metadata": {
    "id": "eab67f65"
   },
   "outputs": [],
   "source": [
    "# Load Data from .csv file\n",
    "# load_dir = './data/data.csv'\n",
    "load_dir = './2.5d_data/data.csv'\n",
    "# load_dir = './data/debug.csv'\n",
    "data = pd.read_csv(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94daee8",
   "metadata": {
    "id": "b94daee8"
   },
   "source": [
    "### 2.3 Divide for K-fold Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1659336178562,
     "user": {
      "displayName": "Steven Lu",
      "userId": "02385023182250863259"
     },
     "user_tz": 300
    },
    "id": "383a3366",
    "outputId": "4529ac78-cf31-4caf-bca7-54a09ba876fc"
   },
   "outputs": [],
   "source": [
    "# Divide the Dataset for Cross-Validation \n",
    "cv = StratifiedGroupKFold(n_splits=NUM_FOLD, shuffle=True, random_state=14)\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(data, data['empty'], data['case'])):\n",
    "    data.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2866c7e",
   "metadata": {
    "id": "d2866c7e"
   },
   "source": [
    "### 2.4 Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3_seIjJ5WnNr",
   "metadata": {
    "id": "3_seIjJ5WnNr"
   },
   "outputs": [],
   "source": [
    "RESUME = False\n",
    "\n",
    "if RESUME:\n",
    "    # if resume from training, set SKIP_TRAIN as False and set LAST_FOLD, LAST_EPOCH to the next of the loaded checkpoint \n",
    "    # (e.g. LAST_FOLD=2, LAST_EPOCH=6 for loading trained_fold2_epoch5.pth)\n",
    "    # if resume from validation, set SKIP_TRAIN as True and set LAST_FOLD and LAST_EPOCH as in the name of the loaded checkpoint\n",
    "    CKPT_LOAD_PATH = './checkpoints/UNet_batchsize4_fold4/trained/trained_fold3_epoch6.pth'\n",
    "    LAST_FOLD = 3\n",
    "    LAST_EPOCH = 6\n",
    "    SKIP_TRAIN = True\n",
    "else: \n",
    "    LAST_FOLD = 0\n",
    "    LAST_EPOCH = 0\n",
    "    SKIP_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101aea15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 3149,
     "status": "error",
     "timestamp": 1659390925426,
     "user": {
      "displayName": "Steven Lu",
      "userId": "02385023182250863259"
     },
     "user_tz": 300
    },
    "id": "101aea15",
    "outputId": "32d17f61-7903-46f9-8f82-5868fdd6b2e3"
   },
   "outputs": [],
   "source": [
    "fold_loss, fold_dice, fold_iou = [], [], []\n",
    "for fold in range(LAST_FOLD, NUM_FOLD):\n",
    "    # Initialize Modules\n",
    "    DATA_TRANSFORM = Transform(IMG_SIZE).baseline()\n",
    "    MODEL = Model(BACKBONE, PRETRAINED_WEIGHTS, NUM_CLASS, DEVICE).UNet()\n",
    "    LOSS_FN = Loss()\n",
    "    METRIC = Metric(THRESHOLD, EPSILON)\n",
    "    OPTIMIZER = torch.optim.AdamW(MODEL.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # LR_SCHEDULER = torch.optim.lr_scheduler.StepLR(OPTIMIZER, LEARNING_RATE_DROP)\n",
    "    if RESUME:\n",
    "        print()\n",
    "        print(f'---------------- RESUME FROM {CKPT_LOAD_PATH}, LAST FOLD: {LAST_FOLD}, LAST EPOCH: {LAST_EPOCH} ----------------', flush=True)\n",
    "        # load checkpoint\n",
    "        CKPT = torch.load(CKPT_LOAD_PATH)\n",
    "        MODEL.load_state_dict(CKPT['model_state_dict'])\n",
    "        OPTIMIZER.load_state_dict(CKPT['optimizer_state_dict'])\n",
    "        # LR_SCHEDULER.load_state_dict(CKPT['lr_scheduler_state_dict'])\n",
    "        RESUME = False\n",
    "    print()\n",
    "    print(f'----------------------------- FOLD {fold} STARTS -----------------------------', flush=True)\n",
    "    train_loader, valid_loader = get_dataloader(data, fold, DATA_TRANSFORM, TRAIN_BATCH_SIZE, VALID_BATCH_SIZE, NUM_WORKERS)\n",
    "    best_score, best_epoch = 0, 0\n",
    "    epoch_loss, epoch_dice, epoch_iou = [], [], []\n",
    "    for epoch in range(LAST_EPOCH, EPOCH):\n",
    "        print()\n",
    "        print(f'------------------------- EPOCH {epoch} STARTS -------------------------', flush=True)\n",
    "        start_time = time.time()\n",
    "        # training\n",
    "        if not SKIP_TRAIN:\n",
    "            loss = train_one_epoch(train_loader, MODEL, OPTIMIZER, LOSS_FN, DEVICE)\n",
    "            epoch_loss.append(loss)\n",
    "            # LR_SCHEDULER.step()\n",
    "            save_trained_path = f'{CKPT_SAVE_PATH}/trained/trained_fold{fold}_epoch{epoch}.pth'\n",
    "            if os.path.isfile(save_trained_path):\n",
    "                os.remove(save_trained_path)\n",
    "            torch.save({'model_state_dict': MODEL.state_dict(), \n",
    "                        'optimizer_state_dict': OPTIMIZER.state_dict(),\n",
    "                        # 'lr_scheduler_state_dict': LR_SCHEDULER.state_dict()\n",
    "                        }, save_trained_path)\n",
    "        else:\n",
    "            print('------------------ SKIPPED TRAINING ------------------', flush=True)\n",
    "            SKIP_TRAIN = False\n",
    "\n",
    "        # validation\n",
    "        curr_scores = valid_one_epoch(valid_loader, MODEL, METRIC, DEVICE)\n",
    "        dice_score, iou_score = curr_scores[0], curr_scores[1]\n",
    "        epoch_dice.append(dice_score)\n",
    "        epoch_iou.append(iou_score)\n",
    "        if dice_score > best_score:\n",
    "            best_score = dice_score\n",
    "            best_epoch = epoch\n",
    "            save_best_path = f'{CKPT_SAVE_PATH}/best/best_fold{fold}_epoch{epoch}_score{best_score:.3f}.pth'\n",
    "            if os.path.isfile(save_best_path):\n",
    "                os.remove(save_best_path)\n",
    "            torch.save(MODEL.state_dict(), save_best_path)\n",
    "        os.remove(save_trained_path)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'-------------------------- EPOCH {epoch} ENDS --------------------------')\n",
    "        print(f'FOLD: {fold}, EPOCH: {epoch}, TIME ELAPSED: {epoch_time:.3f}, BEST SCORE: {best_score:.3f} BEST EPOCH: {best_epoch}', flush=True)\n",
    "        print()\n",
    "    fold_loss.append(epoch_loss)\n",
    "    fold_dice.append(epoch_dice)\n",
    "    fold_iou.append(epoch_iou)\n",
    "    print(f'------------------------------ FOLD {fold} ENDS ------------------------------', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VR0ALt1Qb9I5",
   "metadata": {
    "id": "VR0ALt1Qb9I5"
   },
   "source": [
    "### 2.5 Plot Loss and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5u_jkZXEVvJE",
   "metadata": {
    "id": "5u_jkZXEVvJE"
   },
   "outputs": [],
   "source": [
    "IMAGE_SAVE_PATH = f'./images/{EXPERIMENT_NAME}_batchsize{TRAIN_BATCH_SIZE}_fold{NUM_FOLD}'\n",
    "if not os.path.exists(IMAGE_SAVE_PATH):\n",
    "    os.makedirs(IMAGE_SAVE_PATH)\n",
    "\n",
    "# Plot Loss\n",
    "for i, loss in enumerate(fold_loss):\n",
    "    plt.plot(loss, label=f'Fold {i}')\n",
    "plt.title('Change of Loss For Each Fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(f'{IMAGE_SAVE_PATH}/loss.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot Dice Coefficients\n",
    "for i, score in enumerate(fold_dice):\n",
    "    plt.plot(score, label=f'Fold {i}')\n",
    "plt.title('Change of Dice Coefficient For Each Fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(f'{IMAGE_SAVE_PATH}/dice_scores.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot IOU Coefficients\n",
    "for i, score in enumerate(fold_iou):\n",
    "    plt.plot(score, label=f'Fold {i}')\n",
    "plt.title('Change of IOU Coefficient For Each Fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(f'{IMAGE_SAVE_PATH}/IOU_scores.pdf')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
